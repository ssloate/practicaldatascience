{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a Dask Cluster on AzureML\n",
    "\n",
    "In this lesson, we'll be using a dask cluster to replicate the [exercise we did in the Big Data section](exercises/Exercise_bigdata.ipynb) where we loaded global temperature data to measure global warming at a number of locations. \n",
    "\n",
    "To run this code, in addition the `dask` and `pandas`, which you should already have installed, you'll need to install the following packages (`azureml-sdk` and `dask_cloudprovider`) with the following commands:\n",
    "\n",
    "```\n",
    "conda install -c conda-forge azure-storage-blob # For managing storage\n",
    "pip install azureml-sdk                         # For managing compute\n",
    "pip install dask_cloudprovider=0.4.1\n",
    "```\n",
    "\n",
    "Note that `dask_cloudprovider` sometimes doesn't load the right version if you don't specify, and as of October 2020 the right version isn't even on `conda-forge`, so don't use `conda install`. You can also pip install `azure-storage-blob` if you prefer `pip` to `conda`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload our Data\n",
    "\n",
    "We'll start by uploading our Climate Data to Azure storage. The one thing you don't see in this code is that I've already created a Storage Account with Azure, and I put the \"connection string\" for that account into the file that I'm reading (you can put it directly into your code, but if I did that y'all could see my connect string and mess with my account!). \n",
    "\n",
    "You can get the connection string by going to your Azure Portal, selecting Storage Accounts, selecting the relevant account, and clicking \"Access Keys\" on the left. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, __version__\n",
    "\n",
    "# Load connection string\n",
    "con\n",
    "\n",
    "# Create the BlobServiceClient object which will be used to create a container client\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "\n",
    "# Create a unique name for the container\n",
    "container_name = \"quickstart\" + str(uuid.uuid4())\n",
    "\n",
    "# Create the container\n",
    "container_client = blob_service_client.create_container(container_name)\n",
    "# Create a blob client using the local file name as the name for the blob\n",
    "blob_client = blob_service_client.get_blob_client(container=container_name, blob=local_file_name)\n",
    "\n",
    "print(\"\\nUploading to Azure Storage as blob:\\n\\t\" + local_file_name)\n",
    "\n",
    "# Upload the created file\n",
    "with open(upload_file_path, \"rb\") as data:\n",
    "    blob_client.upload_blob(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "from azureml.core import Workspace, Experiment\n",
    "from dask_cloudprovider import AzureMLCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the workspace from configuration file.\n",
    "# You can also do this by specifing your subscription ID,\n",
    "# resource_group, and workspace_name, but those are\n",
    "# sensitive so I'm using a config file I can exclude from\n",
    "# this repo. See Workspace docstring for details.\n",
    "\n",
    "ws = Workspace.from_config(\"azure_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - If 'script' has been provided here and a script file name has been specified in 'run_config', 'script' provided in ScriptRunConfig initialization will take precedence.\n",
      "WARNING - If 'arguments' has been provided here and arguments have been specified in 'run_config', 'arguments' provided in ScriptRunConfig initialization will take precedence.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................................................\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - If 'script' has been provided here and a script file name has been specified in 'run_config', 'script' provided in ScriptRunConfig initialization will take precedence.\n"
     ]
    }
   ],
   "source": [
    "amlcluster = AzureMLCluster(\n",
    "    ws,\n",
    "    vm_size=\"STANDARD_DS13_V2\",  # Azure VM size for the Compute Target\n",
    "    datastores=ws.datastores.values(),  # Azure ML Datastores to mount on the headnode\n",
    "    environment_definition=ws.environments[\n",
    "        \"AzureML-Dask-CPU\"\n",
    "    ],  # Azure ML Environment to run on the cluster\n",
    "    jupyter=True,  # Flag to start JupyterLab session on the headnode\n",
    "    initial_node_count=2,  # number of nodes to start\n",
    "    scheduler_idle_timeout=7200,  # scheduler idle timeout in seconds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e709b67abd414b996786d0893a71c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>AzureMLCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n  â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "amlcluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Your Cluster\n",
    "\n",
    "There are two ways to use your cluster: You can click on the link above to open a connection to JupyterLab running on one of the computers in your cluster, or connect from here with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/distributed/client.py:1130: VersionMismatchWarning: Mismatched versions found\n",
      "\n",
      "+---------+---------------+---------------+---------------+\n",
      "| Package | client        | scheduler     | workers       |\n",
      "+---------+---------------+---------------+---------------+\n",
      "| lz4     | None          | 3.1.0         | 3.1.0         |\n",
      "| numpy   | 1.19.1        | 1.19.2        | 1.19.2        |\n",
      "| python  | 3.7.8.final.0 | 3.6.9.final.0 | 3.6.9.final.0 |\n",
      "+---------+---------------+---------------+---------------+\n",
      "  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\n",
      "WARNING - Retrying (Retry(total=2, connect=2, read=3, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f86333c3d10>: Failed to establish a new connection: [Errno 60] Operation timed out')': /history/v1.0/subscriptions/bab5ec70-4531-48dc-9d25-2d73206d5741/resourceGroups/nce8/providers/Microsoft.MachineLearningServices/workspaces/nce8/experiments/dask-cloudprovider/runs/dask-cloudprovider_1601918130_97409214\n",
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "ERROR - _GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "concurrent.futures._base.CancelledError\n",
      "WARNING - Retrying (Retry(total=2, connect=2, read=3, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f867ab95dd0>: Failed to establish a new connection: [Errno 60] Operation timed out')': /history/v1.0/subscriptions/bab5ec70-4531-48dc-9d25-2d73206d5741/resourceGroups/nce8/providers/Microsoft.MachineLearningServices/workspaces/nce8/experiments/dask-cloudprovider/runs/dask-cloudprovider_1601918130_97409214\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "c = Client(amlcluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you're off to the races!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data from Azure\n",
    "\n",
    "If your data is CSV or parquet... \n",
    "\n",
    "Load `adlfs`:\n",
    "\n",
    "```\n",
    "conda install -c conda-forge adlfs\n",
    "```\n",
    "\n",
    "Then just put your account data in a dictionary, put `az` at start of reads, and use the `storage_options`. \n",
    "\n",
    "```\n",
    "import dask.dataframe as dd\n",
    "\n",
    "storage_options={'account_name': ACCOUNT_NAME, 'account_key': ACCOUNT_KEY}\n",
    "\n",
    "ddf = dd.read_csv('az://{CONTAINER}/{FOLDER}/*.csv', storage_options=storage_options)\n",
    "ddf = dd.read_parquet('az://{CONTAINER}/folder.parquet', storage_options=storage_options)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
